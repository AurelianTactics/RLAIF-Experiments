Rough Goal
Explore how LLMs can be used to help with reward functions in RL agents. Can have applications to RLAIF, RLHF, LLMs, RL.

working

	need to reassess the feasibility of this
		would need a pipeline to get the reward model I think (unless I vastly oversimplify)
		need a better env for this wehre it's actually useful

	TEST
		collect trajectory


	query LLM
		DONE need to see what blackjack output looks like in terms of caption for bust, split etc
		DONE copy from paper to demonstrate konwledge
		create a reward function
			DONE write 5 prompts
			can try Bard and ChatGPT 3.5 for reward function
			
		prompt as the paper does for captioning

		need a function for captioning

		need base prompt taht can be filtered in

		since limited number of states, maybe just simplify and design the all possible
			hopefull functional way to get from data so can use in other envs

		then need the LLM output
			will save somewhere

		need code to query and save
			probably batch, probably a public library for this

		probably want to review motif code more for this
			make sure good with the output can be used how you want
			maybe just small dry run for next step

		optional: caption with the action too maybe? not motif like but could be useful
			idk, I see why it doesn't make sense in the paper and why it does here because here have full obs
			and that becomes the caption.		
		
	train reward function
		DONE read baseline motif code

		maybe use the vertix AI pipeline for this
			https://learn.deeplearning.ai/reinforcement-learning-from-human-feedback/lesson/4/tune-an-llm-with-rlhf
			would want a way to just get the reward model, don't care about the tuning of the LLM
		probably want to do more research in general to see what tools are out there to simplify this

	train agent

	present results

	future steps
		how to pipeline it
		how to integrate cloud


Ideas
Go through motif repo and understand better
organize motif thoughts and possible ideas
maybe switch to mountaincar

maybe research some other prompt suggestions

simple example envs
	nothing captioning like motif (I don't think) but could create captions
		blackjack has them built in (sort of)
	or use trajectories (paper mentioned like that)
	https://www.gymlibrary.dev/environments/classic_control/
	https://www.gymlibrary.dev/environments/box2d/

RLAIF have it design a function

lot of prompt engineering that can be done

Reproducing Motif on different envs
	need a good env for it
		ideas: botbowl, something with a play-by-play

simple motif framework
	collect env trajectories.
		want it to be diverse It hink. doesn't need to be expert only
		motif does caption plus score
			would want to store like episode, step, actions, rewards, obs, info, done, captions, other relevant info (like how it's gathered, random seed etc)
			could do more important stuff later and just do caption for now
	design LLM prompt
		confirm has basic knowledge of the env
		base prompt plus how it will score
		ablations
		when not better than others
	query LLM
		how to send to the API in python
			chat gpt and bard
			probably batch mode as well
			maybe tips on speeding itup
		send base prompt plus two captions
			not sure how the pairs are picked. maybe some elo like sampling? idk
		confirm prompt answerings are showing something positive
		record answers
		run through LLM for scoring (does this happen later or feedback into waht is prompted)
	train agent on env with new reward function
		intrinsic vs. extrinsic balance
		hyperparam tunes: 
			reward balance, threshold of what is sure or not
		me: confirm motif does this because i'm not sure about how online it is
	experiment results and presentation
		against baseline/oracle/other contenders/ablations

Project wrap up
	update the rlease files

To read

Read Papers on RLAIF
Motif https://arxiv.org/abs/2310.00166
	LLM sets preferences on paired samples. used to train a reward function
Reward Design with Language Models: https://arxiv.org/abs/2303.00001
	RL trajectgories
	feed trajectories and other info into LLM to get a rewards
https://arxiv.org/abs/2302.06692
	ELLM
	Exploration LLM. Using LLM to generates subgoals to help RL agent explore

copilot questions
	way to ask for documentation inline rather than ask
		something about /doc
	way to accept with shortcut key rather than click

	/tests might do unit tests?


Done

	DONE turn on copilot
		vs studio
		text complete
		ask a question

	DONE see what blackjack looks like
		DONE NO split, double, etc
		https://gymnasium.farama.org/environments/toy_text/blackjack/
			The observation consists of a 3-tuple containing: the player’s current sum, the value of the dealer’s 			DONE one showing card (1-10 where 1 is ace), and whether the player holds a usable ace (0 or 1).
			
			0: Stick
			1: Hit
		
		
	DONE make notes and notebook on blackjack  in notebook
		no split double etc, not doing natural. only stick and hit. no info or captions
		defaults to sab
			I think sab = True but I don't think natural blackjack is covered in this


	DONE fix random agent with rlease set up

	DONE basic motif like thing in blackjack
		collect examples
			DONE see what can be used from rlease
				https://github.com/AurelianTactics/RLease/blob/main/rlease_trajectory_stats.py
				https://github.com/AurelianTactics/RLease/blob/main/rlease_trajectory_builder.py
					maybe not this?

			yeah I think both can be used
				DONE look at usage examples and see what code can be copied over
				think through exactly what I want to save and make sure it is covered
					looks good except need to caption
					either in info or make a caption
						likely have to wrap the env (thinking do this seperate from data collection
				overall usage
				saves stats
					overall saves would be good but periodic is fine I guess
				saves trajectories

	